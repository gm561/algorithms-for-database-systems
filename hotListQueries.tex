\section{Hot list queries}
Gibbons and Matias consider application of concise and counting
samples in the context of choosing $k$ most frequently occurring
values which known as \textit{hot list queries}. Formally it might be
understood as finding $k$ $\langle value, count \rangle$ pairs for
which $count$ is the largest.

\subsection{Algorithms}
Gibbons and Matias \cite{GM98} compared four algorithm for creating
\textit{hot list queries}. The assumption is that we have a relation
$R$ with $n$ tuples and the footprint bound is $m$, where $m \geq 2k$.
%TODO describe confidential threshold $\sigma$

\subsubsection{Traditional samples}
This approach base on obtaining a sample by means of Vitter's reservoir sampling algorithm
\cite{Vit85}. The values in the sample are replaced by pairs $\langle
value, count \rangle$. Then the $k$th largest count $c_k$ is
found. All counts are scaled by $\frac{n}{m}$ and pairs that count is
at least $max(c_k, \sigma)$ are treated as being on the list of hot
queries. It is possible that the sample contains less than $k$
distinct values. In this case less than $k$ different values are
contained on the $hot query list$.

%TODO how the list is maintained

%TODO describe algorithms of Vitter's more precisely
%actually there are four of them it is not mentioned which one is
%chosen.
%TODO this is really unclear what is scaled ? the counts ? before
%after choosing it ? and why they are scaled?

\subsubsection{Concise samples}
Concise sample is obtain using the \textbf{Algorithm
  \ref{alg:offline-concise-algorithm}}.
Then the $k$'th largest count is found. Let us denote it by
$c_k$. The counts are scaled by $\frac{n}{m'}$ where $m'$ is the
sample-size. All pairs that $count$ is at least $max(c_k, \sigma)$ are
treated as \textit{hot queries}. The $count$s are then scaled by $\frac{n}{m'}$ where $m'$
is the sample-size of the concise sample. This algorithm for
$\sigma = 1$ will report all k pairs. For larger $\sigma$ it is
possible that fewer than $k$ values is reported.

\subsubsection{Counting samples}
In this case a sample is found by means of the \textbf{Algorithm
  \ref{alg:maintenace-counting-samples}}. The approach to establish
approximate \textit{hot list queries} is similar to the
one presented above for concise samples. Namely, the $c_k$ is
determined, all pairs that $count$ is at least $max(c_k, \tau - \widetilde{c})$ are
selected. The difference is the methods for scaling the counts.
We need to determine a compensating factor $\widetilde{c}$ for values
prior the first head was flipped. We base the analysis on the
transformation of counting samples into concise sample which gives
$\sigma = 2 - \frac{\widetilde{c}+1}{\tau}$.


\subsubsection{Full histogram on disk}
This algorithm is used as a baseline for the algorithms presented
above. It build a full histogram which means that all possible
$\langle value, count \rangle$ pairs are created and stored. Then the
set of the $k$ most frequently occurring is chosen. The remaining are
still stored and used in presence of update. As the working set of
this algorithm might be of $\O(n)$ it is not considered to be
very useful in real applications. However, it provides precise results
and can be used as a baseline for other methods.

\subsubsection{Confidential threshold $\sigma$}
The approximation methods are intrinsically associated with the
error. To bound it the threshold value $\sigma$ is used. For
traditional methods and concise samples the $\sigma$ is represented by
the integer. It might be understood as the confidence level which
indicates that the value should be reported as a \textit{hot list
  query}. Intuitively the higher the $\sigma$ that less possible is
that a relatively rarely represented value is listed as hot query. On
the other hand the higher it is more likely that the sample provides
less than $k$ values.

\subsubsection{Probabilistic analysis of concise samples}
It is interesting to estimate probability of some events.
First of all let us analyze an event that some frequently occurring value in
relation $R$ will be reported on the hot list queries. Let $v$ denote a
value that occurs $f_v = \frac{\tau \delta}{1 - \varepsilon}$
times. We define $X_{vi}$ to be Bernoulli distributed indicator random
variable for the event that $i$th occurrence of value $v$ is in the
concise sample. We denote threshold by $\tau$. We have for
every $i$ such that $1 \leq i \leq f_v$ that
\begin{align*}
\mathbb{E}[X_{vi}] = \frac{1}{\tau}.
\end{align*}
Let
\begin{align*}
  X = \sum_{i = 1}^{f_v} X_{vi}.
\end{align*}
By linearity of expectation, we have
\begin{align*}
  \mathbb{E}[X]
  = \mathbb{E}[\sum_{i=1}^{f_v} X_{vi}] = \sum_{i=1}^{f_v}
  \mathbb{E}[X_{vi}] = \frac{f_v}{\tau}
\end{align*}
%
Using Chernoff bounds yields
\begin{align*}
  P[X \leq (1-\varepsilon)E[X]] \leq e^{\frac{-\varepsilon^2 E[X]
    }{2}} = e^{\frac{-\delta \varepsilon}{2(1-\varepsilon)}}.
\end{align*}
Hence, we have that
\begin{align*}
  P[X \geq (1-\varepsilon)\mathbb{E}[X]] = 1 - P[X \leq
  (1-\varepsilon)\mathbb{E}[X]] \geq 1 - e^{\frac{-\delta \varepsilon}{2(1-\varepsilon)}}.
\end{align*}
Which means that is not likely to not report a value to the hot list
query that is frequent.
On the other hand it is possible to do similar calculations for the
case when value is infrequent. Let assume that value $v$ occurs at
most $f_v = \frac{\tau\delta}{1-\varepsilon}$ and the threshold equals
$\tau$. Simililarly as above the expectation is computed.
That probability of infrequently occurring value being reported to hot
list query is computed using the Chernoff bound.  Hence, we have
\begin{align*}
P[X \geq (1+\varepsilon)\mathbb{E}[X]] \leq e^{-\frac{\varepsilon^2
    E[X]}{3} } = e^{-\frac{\delta \varepsilon^2}{3(1+\varepsilon)}}.
\end{align*}

\subsubsection{Analysis of counting samples}
The value of $\widetilde{c}$ is determined as follows. Let $c_v$ be a
count for some value $v$ reported in the sample and $f_v$ be the
number of times the value $v$ occurs in the relation. We want to choose
$\widetilde{c}$ such that $Est_v = c_v + \widetilde{c}$ is close to
$f_v$. More precisely we want that $\mathbb{E}[Est_v | v\text{ is in }S] = f_v$. Linearity of
expectation yields
\begin{align*}
  \mathbb{E}[Est_v | v \text{is in} S] = \widetilde{c} + \sum_{i = 1} ^ {f_v}
  (f_v - i + 1) Pr[v \text{ was inserted at the $i$th occurrence $|$ $v$ is
  in $S$}]
\end{align*}
Which after some calculations gives
\begin{align*}
  \widetilde{c} \approx \tau - 1 - f_v \frac{(1/e)^{\frac{f_v}
      {\tau}}} {1 - (1/e)^{\frac{f_v}{\tau}}}
\end{align*}
This calculations still does not give an answer how to choose
$\widetilde(c)$ as $f_v$ is not know. Gibbons and Matias suggest to
choose $\widetilde{c}$ as if $f_v = \tau$.

\subsubsection{Number of reported values}
The aforementioned algorithms reports less than $k$ values for certain
distributions. Gibbons and Matias claim that any randomized algorithm
will fail to approximate the frequency exactly. However, the
distribution for which it is likely are the one close to the uniform
distribution and it is rarely interesting to ask for a hot list
queries for this distributions.
