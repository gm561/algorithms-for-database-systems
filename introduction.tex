\section{Introduction}
The increase in data size stored in data warehouse and simultaneous
requirement to obtain query answers within a reasonable time, cause a
need to develop new methods that provide fast but possibly approximate
results. The traditional model rely on a direct access to data
warehouse. In many applications it is too slow due to numerous
operation involving the time costly access to a hard disk. For some applications
obtaining results quickly even if they are only approximated
constitute considerable alternative.
This is the motivation for developing new methods that return quickly
approximate answers. The authors of the paper \cite{GM98} propose a
new model that bases on
approximate answers engine that is responsible for storing sample based summary statistics,
called \textit{synopsis data structures} or \textit{synopses}. Instead
directly to database queries are sent to approximate answers
engine which basing on the synopses return an approximate result.
Working set of the approximate query engine is much slower. Therefore,
it uses high level caches which latency is much lower than a hard disk.
The users after obtaining results can estimate their usefulness and depending on it
decide whether to query proper database in order to obtain precise answers or not.
The novelty of the techniques developed by Gibbons and Matias
\cite{GM98} base on developing a model that provides samples to
approximate answer engine which reflects distribution of
data stored in database and simultaneously maintain its small working set.
This approach differs from the previous ones in which either the
entire database is scanned and the approximate answer is updated as
the scan proceeds or the techniques in which samples set is created in
less memory-efficient way. The goal of Gibbons and Matias \cite{GM98}
is to develop techniques that provide fast response to various type of
queries. This is achievable by placing frequently used structures in
memory close to processor (main memory or caches). From this follows
that it is beneficial to limit size of used data structures. This is
the reason for considering the effectiveness of used data structures
as the function of memory footprint and number of tuples represented
in the sample. Additionally, the measurement of synopsis effectiveness
should takes into account two factors - the
accuracy of the provided answers and the response time. One of the main
difficulty related to data structures providing approximated answers is
keeping them up to data as the content of the  data warehouse changes.
In this case the algorithms responsible for maintaining appropriate
structures in presence of inserting and deleting tuples has to be
developed.

\subsection{Concise and counting samples}
Gibbons and Matias \cite{GM98} introduced two new sampling summary
statistics, \textit{concise samples} and \textit{counting
  samples}. The basic idea seems to be simple. However,
they also developed techniques to fast incremental maintenance of this
structure which in face of frequent updates is not trivial.

The idea behind the \textit{concise samples} and \textit{counting
  samples} is to focus on the class of queries that ask for a one or
more attributes of data relation. The most rudimentary synopsis data
structure is selecting a random set of tuples in the relation. Gibbons
and Matias point out that the frequently occurring values lead to
inefficiency and it representation is not optimal. Instead they
proposed to represent frequently occurring values as a pair
\textit{<v,k>} where \textit{v} represents value and \textit{k}
number of value copies in the sample. If we assume that value is
represented by a type that size is equal to an integer numeric
type. Then we save space for $ k - 2 $ points. This simple idea is a
base for concise sample data structure. Which is defined as a uniform
random sample of the data set such that values appearing more than
once in the sample are represented as a value and a count. Gibbons and
Matias argues that however, simple this method might be powerful as
freeing up space let move data to the memory that is closer to the
processor which substantially reduce access time. What is more this
technique is never worse (in the sense of occupied memory) that
rudimentary random samples. The method used to represent concise
samples might be also seen as a way to improve the quality of
approximation answer, as the saved memory might be used for additional
samples. Hence, the result is more accurate.

The number of point and \textit{<v,k>} pairs depends on the data
distribution. The approximation engine has to keep track of inserts
and deletion operation and do corresponding updates in its concise
sample set. It is not easy as in general it can not be easily known
(without scanning entire database relation) when value becomes
important enough and should be represented in the sample (as it was
not tracked before). Gibbons and Matias suggest a probabilistic
algorithm that that maintains concise sample set within fixed memory
footprint bound.

The second technique proposed in the paper \cite{GM98} are
\textit{counting samples}.
%TODO describe count samples


\subsection{Previous techniques}
By the 1998 there were known some other techniques for gathering
approximate answers. On of them was \textit{online aggregation}
proposed by Hellerstein, Hass and Wang. This framework based on
scanning database on random. The approximate answer is being developed
and displayed to the user together with confidence interval. It is
possible to stop the scan in any time. This approach differs from the
one proposed by Gibbons and Matias \cite[GM98] that base on
precomputed structures which are synopses. Although, synapses are not
%TODO be more elaborate on that
sufficient for all types of aggregation queries the are much faster as
do not require neither disk accesses which might be slow nor special
techniques needed to develop access in random order (required for good
approximation accuracy).
%describe other techniques know to the authors of the paper

