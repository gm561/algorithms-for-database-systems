\section{Introduction}
The increase in data size stored in data warehouse with simultaneous
requirement to obtain query answers within a reasonable time created a
need to develop new methods that provide fast approximate
results. Models that base entirely on direct access to a data
warehouse turn out to be not efficient enough due to numerous
operation involving slow disk. There are applications
for which obtaining results quickly, even if they are only
approximation, is far better than waiting for slow but exact
answers. That is the reason for focusing on fast, approximated methods that
constitute a considerable alternative.

Gibbons and Matias \cite{GM98} developed new techniques that try to meet a
need of users that requires fast, approximated answers. They proposed techniques that use not
only data warehouse but also an approximate answer engine.
The approximate answer engine is responsible for storing
sample based summary statistics, called \textit{synopsis data
  structures} or \textit{synopses}. Instead
directly to a data warehouse user queries are sent to approximate answer
engine which using \textit{synopses} returns approximate results
together with accuracy measure.
The user can estimate usefulness of the query answer and depending on it
decide whether to query proper database in order to obtain exact results or not.

The main advantage of approximate query engine is smaller working set.
Therefore, it can use fast high level memory to return query answers
which results in much quicker response.

The novelty of techniques developed by Gibbons and Matias
\cite{GM98} base on developing models that provides data sample
which better reflects distribution of
data stored in a database and simultaneously maintain small working set.
This techniques differs from the previous ones in which either the
entire database is scanned and the approximate answer is updated as
the scan proceeds or in which data sample is created in the less
memory efficient way.

The crucial observation is the fact that techniques that provide
fast response to various type of queries might be developed if the
frequently used structures are stored in high level memory
(i.e. memory close to processor like caches or main memory). From this follows
that it is beneficial to find a way to limit size of used data structures. This is
the reason for considering the effectiveness of used data structures
as the function of memory footprint and number of tuples represented
in the sample. Additionally, the measurement of effectiveness
should take into account two factors - the
accuracy of the provided answers and the response time.

Main difficulty related to data structures providing approximated answers is
keeping them up to data as the content of the data warehouse changes.
Gibbons and Matias \cite{GM98} proposed algorithms that let
maintain synopses data structures up to date in presence of insertion
and deletion of tuples from data warehouse.

\subsection{Concise and counting samples}
Gibbons and Matias \cite{GM98} introduced two new sampling based summary
statistics - concise samples and counting
  samples. However the basic idea behind this methods is simple, it
also requires development of algorithms for fast incremental
maintenance of this structures in face of frequent updates of data
warehouse.

The concise samples and counting samples focus on the class of queries
that ask for a one or more attributes of a single data relation.

The previous solutions like the one proposed by Vitter \cite{Vit85}
base on selecting a random set of tuples from the relation.
This sample set establish an approximation of distribution of values
in a relation. The quality of the representation depends on the size of
the sample. Gibbons and Matias observed that the frequently occurring values in the
sample lead to inefficient use of space and hence, it representation
is not optimal.
They proposed an alternative way to represent frequently occurring
value in the sample as a pair $\langle value, count \rangle$ where
$value$ is a value of an attribute in a relation and $count$ is the
number of this value copies in the sample. If the attribute value is
represented by a type that size is equal to an integer numeric
type the same that is used to represent $count$, then a single pair
save space for $count - 2$ additional points.

This idea is a base for concise sample data structure. Formally,
concise sample is defined as a uniform
random sample of the data set such that values appearing more than
once in the sample are represented as a value and a count pair which
is denoted further as $\langle value, count \rangle$. If the
value occurs exactly once in the sample it is represented as a
singleton value, is denote by $value$ and its size is the same as a
single the size of a single data point.

% Gibbons and
% Matias argues that this simple method is powerful as
% freeing up space let move data to the memory that is closer to the
% processor which substantially reduce access time. What is more, this
% technique is never worse (in the sense of occupied memory) that
% rudimentary random samples. The method used to represent concise
% samples might be also seen as a way to improve the quality of
% approximation answer, as the saved memory might be used for additional
% samples. Hence, the result is more accurate.

The second technique proposed in the paper \cite{GM98} are
counting samples. They are conceptually similar to concise
samples as they also make use of representation of frequently
occurring values in the samples as $\langle value, count \rangle$ but
provides more flexibility. In particular counting samples work also in
presence of delation of tuples from the data warehouse which is not
possible when concise samples are used.

\subsection{Previous techniques}
Already before 1998 when Gibbons and Matias published their paper
there had been known some techniques for obtaining
approximate query answers. One of them was online aggregation
proposed by Hellerstein, Hass and Wang \cite{HHW97}. This
framework based technique relaying on scanning database on random.
The approximate answer is being developed
and displayed to the user together with confidence interval. It is
possible to stop the scan in any time. This technique requires access
to disk memory and algorithms to access tuples of
relation randomly, which makes it less efficient.

Another approach proposed by Vitter \cite{Vit85} is
similar to concise and counting samples. It creates a sample of tuples
of a relation. Nevertheless, it less memory efficient as each value is
represented independently and does not provide accumulated pairs as
in the methods developed by Gibbons and Matias.

