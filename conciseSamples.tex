\section{Concise Samples}

The number of $\langle value, count \rangle$ pairs or singletons
$values$ that form the representation of a concise sample depends on the data
distribution in data warehouse. The approximation engine has to keep track of inserts
operation and when the occurs update correspondingly its data structures.
The main difficultly is to recognize, without scanning
entire data warehouse, when a not tracked value becomes important enough to be
represented in the sample. Gibbons and Matias suggest a probabilistic
algorithms that maintains concise sample set within fixed memory
footprint bound.

As it was already mentioned in the introduction the concise sample
consists of two types of structures. It is
either a pair $\langle value, count \rangle $ or a singleton value. The first
type is used for values that occurs in the sample more than once
(i.e $count > 1$). The values that are present only once in the sample
are represented by singleton $value$. The motivation for this
approach is saving space. As a result sample size may be
much larger than memory footprint of concise sample. Let $S$ denote
concise sample then we have
\begin{align*}
  S = {\langle v_1, c_1 \rangle, \dots, \langle v_j, c_j \rangle,
    v_{j+1}, \dots, v_l }
\end{align*}
is a concise sample where values between $1$ and $j$ are $\langle
value, count \rangle$ pairs and values $v_{j+1}, \dots, v_l$ are
singletons, then the we define $sample \hyph size(S)$ as
\begin{align*}
sample \hyph size(S) = l - j + \sum_{i=1}^j c_i
\end{align*}
and memory $footprint(S)$ as
\begin{align*}
footprint(S) = l + j.
\end{align*}

Gibbons and Matias focus mostly on random
sample sets of a single attribute of a relation.
They claim that it is possible to generalize this
approach for samples sets with pair or more attributes. However, it
leads to much bigger values space i.e $ |R.A_0| \times |R.A_1| \times
\dots |R.A_N|$ where $|R.A_i|$ denotes the number of possible values taken by
attribute $A_i$ in relation $R$. A concise sample is treated as a uniform sample of
size $sample \hyph size(S)$ and might be used in any sample-base method for
providing approximate query answers.

\subsection{Algorithm for offline computation of concise samples }
Gibbons and Matias \cite{GM98} propose an algorithm that extracts
concise samples from the static relation residing on a disk.
Let $n$ denote the number of tuples in the relation, $m$ the
maximum footprint, $t$ a tuple in relation $R$, $t.A$ an attribute $A$
of tuple $t$ and $S$ a concise sample. The following algorithm
compute a concise sample

\begin{center}
  \captionof{algorithm}{Offline concise sample computation.}
    \label{alg:offline-concise-algorithm}
\begin{algorithmic}
  \State $m \gets$ maximum footprint
  \State $n \gets$ number of tuples in the relation
  \State $S \gets  \varnothing$
  \While{$footprint(S) \leq m $ or $sample \hyph size(S) \leq n$}
    \State $t \gets$ randomly chosen tuple from the relation $R$
    \If {$t.A$ is represented by singleton value in $S$}
        \State remove singleton $t.A$ from S.
        \State put pair $\langle t.A, 2 \rangle$ to $S$
    \ElsIf {$t.A$ is represented by a pair in $S$}
        \State increase $count$ in $\langle t.A, count
        \rangle \gets$ by one.
    \Else
        \State add singleton $\angle t.A \rangle$ to $S$
    \EndIf
    \EndWhile
  \end{algorithmic}
\end{center}

Let $m'$ be the sample-size of $S$. Then the above algorithm requires
$O(m')$ number of disk accesses. This algorithm might be seen
as drawing with replacement - the once drawn tuples are
not removed and can be drawn again. As a result of the execution of
Algorithm \ref{alg:offline-concise-algorithm} we obtain
a sample that is an approximation of tuples distribution in the relation.

\subsection{Algorithm for online incremental maintenance of concise samples}
The Algorithm \ref{alg:offline-concise-algorithm} requires
linear number of disk accesses. As a result updating the structures is slow. That is why
Gibbons and Matias \cite{GM98} present an online algorithm that requires no disk
accesses and maintains sampling frequency of concise samples that
reflects distribution of tuples stored in the relation in data base system.
%TODO maybe describe previous approach ?
%TODO read the paper by Vitter and compare this work to it.
The novelty of Gibbons and Matias approach in comparison to previous
works follows from the fact that this algorithm do not assume that
sample-size is know up-front (it can not be determined as it differs from the footprint).
The sample-size depends not only on the footprint but also on data
distribution. As distribution in general is not known at the beginning
and might change in time, it is actually not possible to forseen how many different
samples will be stored. The following algorithm maintains concise
samples during insertion

\begin{center}
  \captionof{algorithm}{Online incremental maintenance of concise samples.}
    \label{alg:maintenance-concise-algorithm}
    \begin{algorithmic}
    \LineComment{$\tau$ - entry threshold}
    \LineComment{$t$ new added tuple}
    \LineComment{$S$ current concise sample}
    \Function{AddNewTuple}{$\tau$, $S$, $t$}
    \State {$isAdded \gets$ true  with probability $\frac{1}{\tau}$,
      false otherwise}
    \If{$isAdded = true$ and $t$ not present in $S$}
    \State {create a singleton value $t.A$ and add it to $S$ }
    \ElsIf{$isAdded = true$ and $t.A$ is a singleton in $S$}
    \State {remove singleton $t.A$ from $S$}
    \State {add $\langle t.A, 2 \rangle$ to $S$}
    \ElsIf{$isAdded = true$ and $t$ present in $S$}
    \State {increase $count$ by one in the pair $\langle t.A, count \rangle$}
    \EndIf
    \If{footprint of $S > m$}
    \State{call MakeSpace()}
    \EndIf
    \EndFunction
    \State
    \Function{MakeSpace}{}
    \State {Raise the threshold to some $\tau'$}
    \ForAll{$s$ in $S$}
    \If{$s$ is a singleton value in $S$}
    \State {remove s from $S$ with probability
      $\frac{\tau'}{\tau}$}
    \ElsIf{$s$ is a pair $\langle value, count \rangle$ in $S$}
    \State{$count' \gets 0$}
    \For{$i = 1 \to count$}
    \State{increase $count'$ with probability $\frac{\tau'}{\tau}$}
    \EndFor
    \State{$count \gets count - count'$}
    \If{$count = 0$}
    \State{remove $\langle value, count \rangle$ from $S$}
    \ElsIf{$count = 1$ and $value$ is represented as a pair}
    \State{Remove pair $\langle value, count \rangle$ from $S$}
    \State{Insert singleton $value$ into $S$}
    \EndIf
    \EndIf
    \EndFor
    \State {$\tau \gets \tau'$}
    \EndFunction
  \end{algorithmic}
\end{center}

%TODO add more detail description of the algorithm.
The key invariant of the \textbf{Algorithm
  \ref{alg:maintenance-concise-algorithm}} is that each tuple is in
represented in concise sample as if the current value of the threshold
$\tau$ were always constant. In other words probability of a tuple being
represented in concise sample is equal $\frac{1}{\tau}$ even if the
$\tau$ has been changing in the past. Gibbons and Matias \cite{GM98} provide proof
of this invariant which seems to be not fully correct. Below this
proof has been sketch.
\begin{theorem}
Each tuple in the relation is treated as the invariant was always
$\tau$ even if it has changed from some $\tau'$ to $\tau$.
\end{theorem}
\begin{proof}
Let $X_{t\tau}$ be an indicator random variable of the event that tuple
t is in the sample when the threshold is $\tau$. Clearly, we have
\begin{align*}
P(X_{t\tau} = 1) = \frac{1}{\tau}
\end{align*}
The probability of the tuple $t$ to be in the sample after changing a
threshold to $\tau'$ equals

\begin{align*}
  P(X_{t\tau'} = 1) &= P(X_{t\tau'} = 1 | X_{t\tau} = 1) P(X_{t\tau} = 1)
  + P(X_{t\tau'} = 1 | X_{t\tau} = 1) P(X_{t\tau} = 1) \\
  &=  P(X_{t\tau'} = 1 | X_{t\tau} = 1) P(X_{t\tau} = 1) \\
  &= \frac{\tau}{\tau'} \frac{1}{\tau} = \frac{1}{\tau'}.
\end{align*}
\end{proof}
Hence, if the current threshold is $\tau$, the probability of a tuple
to be in the sample equals $\frac{1}{\tau}$ . The only problem
with this proof is that the authors in \cite{GM98} define the
probability of evicting a sample to be
\begin{align*}
P(X_{t\tau'} = 0 | X_{t\tau} = 1) = \frac{\tau}{\tau'}.
\end{align*}
It also follows from the
\textbf{Algorithm \ref{alg:maintenance-concise-algorithm}}. I assume
that this is just a slight inconsistency and in fact a tuple after
change of threshold should
remain in the sample given that it is in the sample before the
threshold was changed with probability $\frac{\tau'}{\tau}$.

It is possible that flipping a coin for each insertion of new data can
create certain overhead. Basing on the reservoir sampling Algorithm X
\cite{Vit85} Gibbons and Matias propose an approach in which a coin is
flipped in order to determine how many consecutive inserts will be
omitted. This idea bases on generating a value for a random
variable with give distribution. As it is known that the probability
of skipping over exactly next $i$ elements is
\begin{align*}
P(\text{skip over exactly next $i$ elements}) = \left( 1 - \frac{1}{\tau}\right)^i\frac{1}{\tau}
\end{align*}
%TODO describe algorihtm for skipping over elements described in
%vitter...
%TODO add an example that show exponential advantage of concise
%samples page 334.

\subsection{Experimental evaluation}
Gibbons and Matias \cite{GM98} compared the Algorithms
\ref{alg:offline-concise-algorithm}, \ref{alg:maintenance-concise-algorithm} with approach
when a random sample is maintained using the reservoir
algorithm, in which case the sample size is equal to the footprint.

The experimental results for the Zpif distribution shows that in
general the overhead connected with choosing a threshold is not
critical. Gibbons and Matias \cite{GM98} used simple model in which
the threshold is increase by $10\%$ of the previous one. It turned out
that even using such a simple model the difference between offline
Algorithm \ref{alg:offline-concise-algorithm} and the maintenance
Algorithm \ref{alg:maintenance-concise-algorithm} are not
big. It would be possible to achieve even better results when for
example a binary search would be used to find an optimal
threshold. 