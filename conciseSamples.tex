\section{Concise Samples}
The goal of the algorithm that maintain set of \textit{concise
  samples} is to obtain a uniform random sample set of $R.A$ values.
The concise sample might be represented by two structures. It might be
either a pair $\langle value, count \rangle $ or a singleton value. The first
type is used for values that occurs in the sample set more than once
(i.e $count > 1$). The values that are present only once in the sample
set are represented just by $value$. The motivation for this
approach is obviously saving space. As a result for sample size may be
much larger than memory footprint of concise sample. We have that if 
\begin{align*}
  S = {\langle v_1, c_1 \rangle, \dots, \langle v_j, c_j \rangle,
    v_{j+1}, \dots, v_l }
\end{align*}
is a concise sample where values between $1$ and $j$ are $\langle
value, count \rangle$ pairs and values $v_{j+1}, \dots, v_l$ are
singletons, then the we define $sample-size(S)$ as
\begin{align*}
sample-size(S) = l - j + \sum_{i=1}^j c_i
\end{align*}
and memory $footprint(S)$ as 
\begin{align*}
footprint(S) = l + j.
\end{align*}

The approach presented by Gibbons and Matias focuses mostly on random
sample sets of a single attribute of some relation
i.e. \textit{R.A}. They claim that it is possible to generalize their
approach for samples sets with pair of attributes. However, it
leads to much bigger values space i.e $ |R.A0| \times |R.A1|$
where by $|R.Ai|$ we denote the number of possible values taken by
attribute $R.Ai$. A concise sample is treated as a uniform sample of
size $sample-size(S)$ and might be used in any sample-base methods for
providing approximate query answers.

\subsection{Offline/static computation concise samplesalgorithm}
Gibbons and Matias \cite{GM98} proposed the an algorithm that extracts
concise samples from the static relation residing on a disk.
By $n$ we denote the number of tuples in the relation, $m$ is the
maximum footprint and $S$ is a concise sample set.

\begin{center}
  \captionof{algorithm}{Offline/static concise sample computation}
    \label{alg:offline-concise-algorithm}
\begin{algorithmic}
  \State $m \gets$ maximum footprint
  \State $n \gets$ number of tuples in the relations
  \State $S \gets  \varnothing$
  \While{$footprint(S) \leq m $ or $sample-size(S) \leq n$}
    \State $T \gets$ randomly chose tuple from the relation $R$
    \State $value(A) \gets$ extract attribute $R.A$ from tuple $T$
    \If {$value(A)$ is represented by singleton value in $S$}
        \State remove singleton $value(A)$ from S.
        \State put pair $\langle value(A), 2 \rangle$ to $S$
    \ElsIf {$value(A)$ is represented by a pair in $S$}
        \State increase $count$ in $\langle value(A), count
        \rangle \gets$ by one.
    \Else
        \State add singleton $\angle value(A) \rangle$ to $S$
    \EndIf
    \EndWhile
  \end{algorithmic}
\end{center}

Let $m'$ be the sample-size of $S$. Then the above algorithm requires
$\Theta(m')$ number of disk accesses. This algorithm might be seen
as drawing with replacement (i.e. the chosen tuples are
not removed from the relation). As a result of its execution we obtain
an approximation of tuples distribution in the relation.

\subsection{Incremental maintenance of concise samples}
The \textbf{Algorithm \ref{alg:offline-concise-algorithm}} requires
linear number of disk
accesses. As a result updating the structures is slow. That is why
the \cite{GM98} presents an online algorithm that requires no disk
accesses and maintains sampling frequency of concise samples that
reflects distribution of tuples stored in the relation in data base system.
%TODO maybe describe previous approach ?
%TODO read the paper by Vitter and compare this work to it.
The novelty of Gibbons and Matias approach in comparison to previous
works follows from the fact that this algorithm do not assume that
sample-size is know up-front (i.e. it differs from the footprint).
The sample-size depends not only on the footprint but also on data
distribution. As distribution in general is not known at the beginning
and might change in time, it is actually not known how many different
samples will be stored. The following algorithm maintains concise
samples during insertion

\begin{center}
  \captionof{algorithm}{Incremental maintenance of concise samples.}
    \label{alg:maintenance-concise-algorithm}
    \begin{algorithmic}
    \LineComment{$\tau$ - entry threshold}
    \LineComment{$t$ new added tuple}
    \LineComment{$S$ current concise sample}
    \Function{AddNewTuple}{$\tau$, $S$, $t$}
    \State {$isAdded \gets$ true  with probability $\frac{1}{\tau}$,
      false otherwise}
    \If{$isAdded = true$ and $t$ not present in $S$}
    \State {create a singleton value $t.A$ and add it to $S$ }
    \ElsIf{$isAdded = true$ and $t.A$ is a singleton in $S$}
    \State {remove singleton $t.A$ from $S$}
    \State {add $\langle t.A, 2 \rangle$ to $S$}
    \ElsIf{$isAdded = true$ and $t$ present in $S$}
    \State {increase $count$ by one in the pair $\langle t.A, count \rangle$}
    \EndIf
    \If{footprint of $S > m$}
    \State{call MakeSpace()}
    \EndIf
    \EndFunction
    \State
    \Function{MakeSpace}{}
    \State {Raise the threshold to some $\tau'$}
    \ForAll{$s$ in $S$}
    \If{$s$ is a singleton value in $S$}
    \State {remove s from $S$ with probability
      $\frac{\tau'}{\tau}$}
    \ElsIf{$s$ is a pair $\langle value, count \rangle$ in $S$}
    \State{$count' \gets 0$}
    \For{$i = 1 \to count$}
    \State{increase $count'$ with probability $\frac{\tau'}{\tau}$}
    \EndFor
    \State{$count \gets count - count'$}
    \If{$count = 0$}
    \State{remove $\langle value, count \rangle$ from $S$}
    \ElsIf{$count = 1$ and $value$ is represented as a pair}
    \State{Remove pair $\langle value, count \rangle$ from $S$}
    \State{Insert singleton $value$ into $S$}
    \EndIf
    \EndIf
    \EndFor
    \State {$\tau \gets \tau'$}
    \EndFunction
  \end{algorithmic}
\end{center}

%TODO add more detail description of the algorithm.
The key invariant of the \textbf{Algorithm
  \ref{alg:maintenance-concise-algorithm}} is that each tuple is in
represented in concise sample as if the current value of the threshold
$\tau$ were always constant. In other words probability of a tuple being
represented in concise sample is equal $\frac{1}{\tau}$ even if the
$\tau$ has been changing in the past. Gibbons and Matias \cite{GM98} provide proof
of this invariant which seems to be not fully correct. Below this
proof has been sketch.
\begin{theorem}
Each tuple in the relation is treated as the invariant was always
$\tau$ even if it has changed from some $\tau'$ to $\tau$.
\end{theorem}
\begin{proof}
Let $X_{t\tau}$ be an indicator random variable of the event that tuple
t is in the sample when the threshold is $\tau$. Clearly, we have
\begin{align*}
P(X_{t\tau} = 1) = \frac{1}{\tau}
\end{align*}
The probability of the tuple $t$ to be in the sample after changing a
threshold to $\tau'$ equals

\begin{align*}
  P(X_{t\tau'} = 1) &= P(X_{t\tau'} = 1 | X_{t\tau} = 1) P(X_{t\tau} = 1)
  + P(X_{t\tau'} = 1 | X_{t\tau} = 1) P(X_{t\tau} = 1) \\
  &=  P(X_{t\tau'} = 1 | X_{t\tau} = 1) P(X_{t\tau} = 1) \\
  &= \frac{\tau}{\tau'} \frac{1}{\tau} = \frac{1}{\tau'}.
\end{align*}
\end{proof}
Hence, if the current threshold is $\tau$, the probability of a tuple
to be in the sample equals $\frac{1}{\tau}$ . The only problem
with this proof is that the authors in \cite{GM98} define the
probability of evicting a sample to be
\begin{align*}
P(X_{t\tau'} = 0 | X_{t\tau} = 1) = \frac{\tau}{\tau'}.
\end{align*}
It also follows from the
\textbf{Algorithm \ref{alg:maintenance-concise-algorithm}}. I assume
that this is just a slight inconsistency and in fact a tuple after
change of threshold should
remain in the sample given that it is in the sample before the
threshold was changed with probability $\frac{\tau'}{\tau}$.

It is possible that flipping a coin for each insertion of new data can
create certain overhead. Basing on the reservoir sampling Algorithm X
\cite{Vit85} Gibbons and Matias propose an approach in which a coin is
flipped in order to determine how many consecutive inserts will be
omitted. This idea bases on generating a value for a random
variable with give distribution. As it is known that the probability
of skipping over exactly next $i$ elements is
\begin{align*}
P(\text{skip over exactly next $i$ elements}) = \left( 1 - \frac{1}{\tau}\right)^i\frac{1}{\tau}
\end{align*}
%TODO describe algorihtm for skipping over elements described in
%vitter...
%TODO add an example that show exponential advantage of concise
%samples page 334.

\subsection{Experimental evaluation}
Gibbons and Matias \cite{GM98} compared the Algorithms
\ref{alg:offline-concise-algorithm}, \ref{alg:maintenance-concise-algorithm} with approach
when a random sample is maintained using the reservoir
algorithm, in which case the sample size is equal to the footprint.

The experimental results for the Zpif distribution shows that in
general the overhead connected with choosing a threshold is not
critical. Gibbons and Matias \cite{GM98} used simple model in which
the threshold is increase by $10\%$ of the previous one. It turned out
that even using such a simple model the difference between offline
Algorithm \ref{alg:offline-concise-algorithm} and the maintenance
Algorithm \ref{alg:maintenance-concise-algorithm} are not
big. It would be possible to achieve even better results when for
example a binary search would be used to find an optimal
threshold. 